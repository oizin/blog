---
title: "Structural Nested Mean Models and G-Estimation: Part I"
description: |
  An introduction to structural nested mean models for causal inference
author:
  - name: Oisin Fitzgerald
    url: 
output:
  distill::distill_article:
    self_contained: false
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(tidyr)
library(splines)
```

# Introduction

## What problem are we solving?   

Causal inference considers questions such as: what would be the impact of giving someone a particular drug? There is a sense of action in a causal question, we are going to **do** something^[hence Pearl's *do* notation [@pearl2009causal]]. An associational question on the other hand, would ask: what is the most likely outcome of someone who is receiving a particular drug? It is passive (but still useful - e.g. predictive modelling). It is generally not the case that the answers to causal and associational questions will be the same, they can even lead to seemingly conflicting results. Most medical research asks a causal question; we wish to inform decisions. It is important we use the appropriate causal methods and thinking to answer the question (see Hernan & Robins @hernan2010causal for an in-depth treatment of this issue). There are many important considerations in answering causal questions from whether the question is worth answering, to what data needs to be collected (see Ahern @ahern2018start for a good outline). This post considers arguably the least important aspect - the statistical methodology used to estimate the causal model parameters. I'll outline (as I poorly understand it) G-estimation of *structural nested mean models (SNMM)* for a single timepoint and two treatment options, and show some simulations along the way. First a brief review of the potential outcomes framework, those familiar with it can skip to the next section.

## Potential outcomes framework

Within the potential outcomes framework causal inference becomes a missing data problem. For the case of two treatment levels $A \in \{0,1\}$ there are two potential outcomes $Y(1)$ and $Y(0)$. If someone gets $A=0$ then the observed outcome is $Y=Y(0)$ (referred to as consistency), and $Y(1)$ is the *counterfactual*. That we only observe one potential outcome per person at a given time is referred to as the *fundamental problem of causal inference* [@holland1986statistics]. Our aim is to calculate the conditional average treatment effect (CATE) $\tau(m) = E(Y(1)-Y(0)|M=m)$ where $M$ is a vector of effect modifiers.

To answer causal questions in a given dataset we require knowledge (or must make assumptions about) about the data generating process - in particular aspects of the treatment decision process (why did the clinical give patient A a particular drug?) and the outcome process (what is it about patient A that increases their risk of a particular outcome?). This knowledge allows us assess whether we can assume conditional exchangeability (unconfoundedness), that within strata of $H$ we can consider treatment to have been randomly assigned. This assumption is key to constructing our estimation process and represents the key difference between causal and associational analysis.

$$Y(a) \perp A |H=h$$
To see this consider the canonical confounding example where sicker patients are more likely to get treated with a particular medicine. A naive comparison of outcomes $E(Y|A=1) - E(Y|A=0)$ amongst patients who did and did not receive the medicine may lead to the impression that treatment is harmful. However, if the treatment decision has been based on an illness severity factor $X_1$ that combines all factors predictive of the disease outcome then we have a biased result

$$E(Y|A=1) - E(Y|A=0) \ne E(Y(1) - Y(0))$$ 

In our example, only by accounting for $X_1$ can we get an unconfounded case where conditional exhangeability holds and the previous inequality becomes an equality. In practise, we will need to make certain assumptions around the data generating process (often reasonable but usually unverifiable) in order to be able to calculate an unbiased treatment difference. 

```{r,fig.cap="Illustration of confounding`. The more severely ill (high X1) are more likely to get treated leading to the situation where the average outcome is worse in the treated."}
knitr::include_graphics("confounding_v2.png")
```

## G-methods Family

Not all statistical approaches that work for treatment comparison at a single timepoint generalise to situations involving time-varying treatments^[where there is treatment confounder feedback]. The G (generalised) methods, developed by James Robins and colleagues - including structural nested mean models (SNMM) [@robins1992g], marginal structural models [@robins2000marginal] and G-computation [@robins1986new], apply in both single and multi-stage treatment effect estimation. If we want to compare the effectiveness of two treatment regimes or policies (e.g. intensive versus standard blood pressure control [@williamson2019effect]) using an observational sources such as electronic medical records understanding the ideas underpinning the G-methods is worthwhile.

# Structural Nested Mean Models

Assume we observe a dataset $Z = (H,Y,A)$ where $H$ is the patient history (covariates; e.g. blood pressure), $Y$ is the outcome of interest and $A$ is the treatment indicator. SNMM involves fitting a model for the conditional treatment contrast $\tau(m) = E[Y(1)-Y(0)|M=m]$ where $Y(a)$ is the potential outcome under treatment $A=a$ and the variables $M$ are effect modifiers. Within medicine it is generally considered reasonable to assume the effect modifying variables $M$ are generally a subset of the history $H$, with the dimension of $M$, $|M|$ possible far smaller than $|H|$. While a large numbers of factors - genetic, environmental and lifestyle - may influence whether someone develops a particular disease their impact on the effectiveness of a particular treatment may be neglible or noisy^[effect modification is a higher order effect]. Nevertheless, for simplcity of notation we will assume $M=H$ from hereon.

As noted, there are several reasons we might be interested in only estimating the contrast verus the outcomes under each treatment directly. One way to think about the observed outcome Y is as being composed of two components, a *prognostic* component and a *predictive* component. The prognostic component determines an individuals likely outcome given a particular medical history, and the predictive component determines the impact of a particular treatment. We can separate the expected outcome for a patient into these components

$$
\begin{align}
E[Y|A=a,H=h] &= E[Y|H=h,A=0] + E[Y(a) - Y(0)|H=h] \\
&= m_0(x) + \gamma(a,h) 
\end{align}
$$

where the setting $A=0$ in $m_0$ corresponds to a control or baseline case. In many cases $m_0(h)$ may be a more complex function than $\tau(x) = \gamma(1,h)$ (Hahn, Murray & Carvalho (2020) @hahn2020bayesian). The potential for misspecification or simple lack of interest (desire for more parsimonious model) in $m_0(h)$ motivates attempting to directly model $\tau(h)$. While disinterest may sound strong, if the final model will be used in practise, and must be explainable to clinical experts, since empircally $\tau(h)$ may be sparser there may be large improvements in interpretability if we model $\tau(h)$ rather than an alternative. The parameter set of $\tau$ is $\psi$ which is what we'll estimate with G-estimation which we turn to next.

# A Review: Independence and Covariance

G-estimation builds upon some basic some facts about conditional independence and covariances which we briefly review. The conditional independence of $X$ and $Y$ given $Z$ is denoted as $X \perp Y |Z=z$. For probability densities $p$ this translates to $p(x,y|z) = p(x|z)p(y|z)$. A direct result of this is that the conditional covariance of $X$ and $Y$ given $Z$ is equal to zero.

$$
\begin{align}
\text{Cov}(X,Y|Z) &= E[(X - E[X|Z])(Y - E[Y|Z])|Z] \\
 &= E[XY|Z] - E[X|Z]E[Y|Z] \\
 &= E[X|Z]E[Y|Z] - E[X|Z]E[Y|Z] \\
 &= 0
\end{align}
$$
Where the third line follows from the ability to factorise the conditional densities $E[XY|Z] = \int \int xy p(xy|z)dxdy = \int x p(x|z) dx \int y p(y|z)dy$. We also note that 1) this holds if we replace $X$ or $Y$ by a function $f$ of $X$ or $Y$ and $Z$, for example $f(X,Z) = X - E(X|Z)$ and 2) relatedly that $E[X(Y - E[Y|Z])|Z] = 0$.

# G-Estimation

G-estimation is an approach to determining the parameters of a SNMM. As we are modelling a treatment contrast in a situation where only one treatment is observed per individual we need a method that accounts for this missingness. There are two explanations below, the second is more general, with some repitition.

## Explanation 1: Additive Rank Preservation

One approach to explaining G-estimation is through assuming additive rank preservation with regard to the treatment effect [@hernan2010causal]. Additive rank preservation is the assumption that the treatment effect is the same for everyone, that $Y(1)-Y(0) = \psi_0$. We emphasise that this is at the individual level (see figure X). As shown later it is not a requirement for G-estimation that this assumption holds, it is expository tool.

```{r,fig.cap="An illustration of rank and additive rank preservation"}
# rank preservations
y0 <- runif(10)
y1r <- exp(0.2*y0)
# additive rank preservation
y1a <- y0 + 0.3

# prepare data
df <- data.frame(yr = c(y0,y1r),
           ya = c(y0,y1a),
           a = c(rep(0,10),rep(1,10)))
df$group <- rep(1:10,2)

p1 <- ggplot(df) +
  geom_point(aes(y=a,x=yr)) +
  geom_line(aes(y=a,x=yr,group=group)) +
  labs(title = "Rank preservation",x = "Y(a)",
       y = "A") + 
  scale_y_continuous(breaks = c(0,1),
                     minor_breaks = NULL) +
  theme_bw()
p2 <- ggplot(df) +
  geom_point(aes(y=a,x=ya)) +
  geom_line(aes(y=a,x=ya,group=group))+
  labs(title = "Additive rank preservation",x = "Y(a)",
       y = "A")+
  theme_bw()+ 
  scale_y_continuous(breaks = c(0,1),
                     minor_breaks = NULL)
grid.arrange(p1,p2)
```

Notice that for the case of additive rank preservation with no effect modification the following holds

$$
\begin{align}
Y(0) &= Y - A[Y(1)-Y(0)] \\
Y(0) &= Y - A\psi \\
\end{align}
$$

If we call this final expression $G(\psi) = Y - A\psi$ then utilising the assumption of unconfoundedness $Y(a) \perp A|X$ this should be uncorrelated with any function $S(A)$ of the treatment assignment mechanism, conditional on the confounders $H$. For this case of no effect modification we'll let $S(A) = A$^[We'll return to choice of $S$ later]. We then have the estimating equation $U(\psi;H,A) = \sum_{n=1}^N G_n(\psi)[A_n - E(A_n|X_n)] = 0$. For the cases where $E(A_n|X_n)$ is unknown we replace it with an estimate. This equation can then be solved for the unknown $\psi$, giving us an approach towards estimation. We continues this case below in **Example 1**.

## Explanation 2: More General Case

Now consider the more general case where $\psi$ is a vector of parameters of our SNMM $\tau_{\psi}(h) = E(Y(1)-Y(0)|H=h)$. Our analog of $G(\psi)$ is now equal to $Y(0)$ in expectation

$$
\begin{align}
E(G(\psi)|H) &=E[Y - A(Y(1)-Y(0))|H] \\
&= E[Y|H] - E[A(Y(1)-Y(0))|H] \\
 &= E[AY(1) + (1-A)Y(0)|H] - E[A(Y(1)-Y(0))|H] \\
 &= E[Y(0)|H]
\end{align}
$$
Where we make use of the consistency assumption $Y = AY(1) + (1-A)Y(0)$ in going from line two to three. As a result we have the following estimating equation $U(\tau;A_n,H_n) = \sum_{n=1}^N G_n(\psi)(S(A_n,H_n)-E[S(A_n,H_n)]) = 0$ for the general case in a single stage setting which is zero in expectation $E(U|L) = 0$. Note that we *could* mean center $G(\psi)$ which we will return to. For the case where $\tau_{\psi}(h)$ can be expressed as a linear model this can be explicity solved, we outline this case below in **Example 2**.

# Some examples

## Example 1

In the case of $\tau(x) = \psi_0$, i.e. $G(\psi_0) = Y - \psi_0A$ and $S(A) = A$ we have an explicit solution

$$
\begin{align}
U(\psi_0;H,A) &= 0 \\
\sum_n^N G(\psi_0) [A - E(A_n|H_n)] &= 0 \\
\sum_n^N [Y_n - \psi_0A_n] [A_n - E(A|H_n)] &= 0 \\
\psi_0 \sum_n^N A_n [A_n - E(A_n|H_n)] &= \sum_n^N Y[A_n - E(A_n|H_n)] \\
\psi_0 &= \sum_n^N \frac{Y_n[A_n - E(A_n|H_n)]}{\sum_n^N A_n [A_n - E(A_n|H_n)]}
\end{align}
$$

As mentioned, in observational studies where the treatment assignment mechanism is unknown we replace $E(A|L)$ (the propensity score) with it's estimate, using e.g. logistic regression or more complex models.

Let's simulating this situation for a model of the form $E(Y(a)|H=h,A=a) = -1.4 + 0.8h_1 + \tau_0a$ where the data is confounded. We'll compare fitting a SNMM for the parameter $\tau_0$ with fitting an linear model for the full conditional expectation. The simulation set up is adapted from Chakraborty and Moodie (2013) [@chakraborty2013statistical]. As shown in figure X both methods return similar results. This is to be expected; the advantage of g-estimation is primarily in situations where the prognostic component $m_0(x)$ is complex and we want a parsimonious model (we show this case below), and in time-varying treatment setting (in part 2).

```{r echo=TRUE}
## SIMULATION 1
M <- 500 # number of rins
tauM <- replicate(M, {
  N <- 100
  # generate data
  x <- runif(N,-0.5,3)
  ps <- function(x) 1/(1 + exp(2 - 1.8*x))
  a <- 1*(ps(x) < runif(N))
  y <- -1.4 + 0.8*x + 2.5*a + rnorm(N)
  # estimate probability of treatment
  pm <- glm(a ~ x,family = binomial())
  ph <- fitted(pm)
  w <- (a-ph)
  # estimate treatment effect
  tauh_g <- sum(w*y)/sum(a*w)
  tauh_lm <- lm(y ~ x + a)$coef["a"]
  c(tauh_g,tauh_lm)
})
```

```{r,fig.cap="A comparison of OLS estimation of the outcome model E(Y|A,H) and G-estimation of the SNMM"}
# plot results
library(ggplot2)
tau_df <- data.frame(t(tauM))
names(tau_df) <- c("SNMM","Outcome model")
tau_df <- tidyr::pivot_longer(tau_df,1:2,names_to = "method",values_to = "estimate")
ggplot(tau_df) +
  geom_histogram(aes(x = estimate),col="white",fill="skyblue") +
  geom_vline(xintercept = 2.5,col="red",linetype=2) +
  theme_bw() +
  facet_wrap(~method,ncol=1) +
  labs(x = "Model estimate")
```

## Example 2

Now consider the linear model with covariates $\tau(h_n) = \psi_0 + \sum_{k=1}^K\psi_k h_{kn}$. The resulting empirical estimating equation is 
 
$$
\begin{align}
U(\psi;A_n,H_n) &= \sum_{n=1}^{N}G(\psi)[S(A_n,H_n)-E(S(A_n,H_n)|L)] \\
U(\psi;A_n,H_n) &= \sum_{n=1}^{N}[Y_n - (\psi_0 + \sum_{k=1}^K\psi_k h_{kn})][S(A_n,H_n)-E(S(A_n,H_n)|L)] \\
\end{align}
$$

As stated this appears to be a single equation with $K+1$ unknowns. However, we can take advantage of $S(A_n,H_n)$ being an arbitrary function to convert it into $K+1$ equations. If we choose $S(A_n,H_n)$ to be the vector valued function $S(A_n,H{*n}) = (1,h_1,,...h_K)^t\cdot A_n$ then we have 

$$
\begin{align}
U_1(\psi;A_n,H_n) &= \sum_{n=1}^{N}[Y_n - (\psi_0 + \sum_{k=1}^K\psi_k x_{kn})][A_n-E(A_n|H_n)] \\
U_2(\psi;A_n,H_n) &= \sum_{n=1}^{N}[Y_n - (\psi_0 + \sum_{k=1}^K\psi_k x_{kn})][A_n-E(A_n|H_n)] h_{1n} \\
 \dots \\
U_K(\psi;A_n,H_n) &= \sum_{n=1}^{N}[Y_n - (\psi_0 + \sum_{k=1}^K\psi_k x_{kn})][A_n-E(A_n|H_n)] h_{Kn} \\
\end{align}
$$
In order to write the above estimating equations in matrix/vector form let $\textbf{H}$ be our $n \times (K+1)$ effect modifier/confounder design matrix, $\textbf{A} = \text{diag}(A_1,A_2,\dots,A_N)$ and $\textbf{W} = \text{diag}(A_1-E[A_1|H_1],\dots,A_N-E[A_N|H_N])$. Then our $K+1$ estimating equations in matrix/vector form are $\textbf{U}(\psi;A_n,H_n) = \textbf{H}^t\textbf{W}(\textbf{y}-\textbf{A}\textbf{H}\boldsymbol{\psi})$. Solving for $\boldsymbol{\psi}$ gives

$$
\begin{align}
\boldsymbol{\psi} &= (\textbf{H}^t\textbf{W}\textbf{A}\textbf{H})^{-1}\textbf{H}^t\textbf{W}\textbf{y}
\end{align}
$$

Simulating this situation, again building the settings off Chakraborty and Moodie (2013) [@chakraborty2013statistical], we have two settings, the $m_0$ component is linear and non-linear. The treatment effect component $\tau(h)$ is always linear - we'll come to non-linar tau next. We compare G-estimation of the SNMM with a (linear) outcome model for the full expectation $E(Y|H,A)$. While this shows the strength of SNMM is not entirely fair as in an empirical setting a simple plot of the data would reveal that a linear model is a bad idea.

```{r echo=TRUE}
gest_snmm <- function(X,y,a,ph) {
  w <- (a-ph)
  W <- diag(w)
  A <- diag(a)
  t1 <- solve(t(X) %*% W %*% A %*% X)
  t2 <- t(X) %*% W %*% y
  t1 %*% t2
}
```

```{r echo=TRUE}
## SIMULATION 2
M <- 500  # number of runs
tauM <- replicate(M, {
  # generate data for linear and non-linear cases
  N <- 100
  x <- runif(N,-0.5,3)
  ps <- function(x) 1/(1 + exp(2 - 1.8*x))
  a <- 1*(ps(x) < runif(N))
  psi0 <- 2.5
  psi1 <- 1.5
  y1 <- -1.4 + 0.8*x + psi0*a + psi1*a*x + rnorm(N)
  y2 <- -1.4*x^3 + exp(x) + psi0*a + psi1*a*x + rnorm(N)
  # estimate probability of treatment
  pm <- glm(a ~ x,family = binomial())
  X <- cbind(rep(1,N),x)
  ph <- fitted(pm)
  # estimate treatment effect
  g1 <- as.vector(gest_snmm(X,y1,a,ph))
  g2 <- as.vector(gest_snmm(X,y2,a,ph))
  ols1 <- lm(y1 ~ x + a + x*a)$coef[c("a","x:a")]
  ols2 <- lm(y2 ~ x + a + x*a)$coef[c("a","x:a")]
  c(g1,g2,ols1,ols2)
})
```


```{r,fig.cap="A comparison of OLS estimation of the outcome model E(Y|A,H) and G-estimation of the SNMM for a linear outcome model.",fig.height = 6}
# plot results
tau_df <- data.frame(t(tauM))
names(tau_df) <- c("g1a","g1ax","g2a","g2ax",
                   "lm1a","lm1ax","lm2a","lm2ax")
tau_df <- tidyr::pivot_longer(tau_df,1:8,
                              names_to = "method",
                              values_to = "estimate")
truth_df2 <- data.frame(method = c("g2a","g2ax",
                                  "lm2a","lm2ax"),
                       estimate = c(2.5,1.5,2.5,1.5))
truth_df1 <- data.frame(method = c("g1a","g1ax",
                                  "lm1a","lm1ax"),
                       estimate = c(2.5,1.5,2.5,1.5))
truth_df1$method_labs <- factor(truth_df1$method,
                             levels = c("g1a","g1ax","lm1a","lm1ax"),
                             labels= c("SNMM:~psi[0]",
                 "SNMM:~psi[1]",
                 "Outcome~model:~psi[0]",
                 "Outcome~model:~psi[1]"))
truth_df2$method_labs <- factor(truth_df2$method,
                             levels = c("g2a","g2ax","lm2a","lm2ax"),
                             labels= c("SNMM:~psi[0]",
                 "SNMM:~psi[1]",
                 "Outcome~model:~psi[0]",
                 "Outcome~model:~psi[1]"))


tau_df$method_labs <- factor(tau_df$method,
                             levels = c("g1a","g1ax","g2a","g2ax",
                   "lm1a","lm1ax","lm2a","lm2ax"),
                             labels= c("SNMM:~psi[0]",
                 "SNMM:~psi[1]",
                 "SNMM:~psi[0]",
                 "SNMM:~psi[1]",
                 "Outcome~model:~psi[0]",
                 "Outcome~model:~psi[1]",
                 "Outcome~model:~psi[0]",
                 "Outcome~model:~psi[1]"))

tau_df %>%
  filter(method %in% c("g1a","g1ax","lm1a","lm1ax")) %>%
  ggplot() +
  geom_histogram(aes(x = estimate),
                 col="white",fill="skyblue") +
  geom_vline(data = truth_df1, aes(xintercept = estimate),
             col="red",linetype=2) +
  theme_bw() +
  facet_wrap(~method_labs,ncol=1,
             labeller = label_parsed) +
  labs(title = "(I) Linear",x = "Model estimate")
```

```{r,fig.cap=,fig.cap="A comparison of OLS estimation of the outcome model E(Y|A,H) and G-estimation of the SNMM for a non-linear outcome model.",fig.height = 6}
# plot results
tau_df %>%
  filter(method %in% c("g2a","g2ax","lm2a","lm2ax")) %>%
  ggplot() +
  geom_histogram(aes(x = estimate),
                 col="white",fill="skyblue") +
  geom_vline(data = truth_df2, aes(xintercept = estimate),
             col="red",linetype=2) +
  theme_bw() +
  facet_wrap(~method_labs,ncol=1,
             labeller = label_parsed) +
  labs(title = "(I) Non-linear",x = "Model estimate")
```

# Standard errors and confidence intervals

Quantifying uncertainty in our parameter estimates is important. For g-estimation of SNMMs the non-parametric bootstrap offers a general approach to estimation of standard errors. If we are interested in a statistic $T(F)$ (e.g. the parameters of a linear model), that can be expressed as a function of the distribtuion of the data $F$ the non-parametric bootstrap repeatedly samples from the empirical distribution $\hat{F}_n$, and calculates $T(\hat{F}_N)$. Given $\hat{F}_N \stackrel{\text{dist}}{\to} F$ as $N \to \infty$ justifies this. In practise this involves repeatedly (e.g. $M=1000$ times) taking a sample of size $n$ with replacement from the observed data $X$ and fitting our model of interest. As we are interested in the distribution of $Y(1)-Y(0)$ we should be careful in the case of an imbalance in the treatment group sizes to resample within each treatment group. 

# More efficient G-estimation

Returning to the idea that setting $U(\psi) = 0$ follows from $\text{Cov}(Y(0),S(A,H)|H) = 0$, it turns out there is a more efficient form of g-estimation if we take full advantage of this idea. Efficient means.... 

Above we use an emprical version of $E(Y(0)(A-S(A|H))|H)=0$ as out estimating equation. That is because we don't know $E(Y(0)|H)$. However, we can estimate it; to do this fit a model and use it to predict $\hat{m}_0(h) = E(Y(0)|H)$ for all observations. Then rather than using $y$ in our estimation procedures we use $\tilde{y} = y-\hat{m}_0(h)$.  The need to estimate several models before G-estimation may seem to add the number of possible sources of error however it actually has the opposite impact. This approach is doubly robust, meaning that if either x or y are correctly specified then the estimated causal effect is unbiased. Correct specification refers to the function form of the model, e.g. aspects such as linear/non-linear effects and interactions. Failure to include confounders would lead to bias even if the model was "correct" for the measured confounders. For our linear model, with m being, the doubly robust estimate is

$$
\begin{align}
\hat{\boldsymbol{\psi}}_e &= (\textbf{H}^t\textbf{W}\textbf{A}\textbf{H})^{-1}\textbf{H}^t\textbf{W}(\textbf{y}-\hat{m}_0)
\end{align}
$$

What is the intuition behind this? We can think of it as information simplification - by taking out $m_0(h)$ we are allowing the estimation to focus on changes with $H$ for $A=0$ is informative about what variation is not due to the treatment effect. Generally noise reduction $\rightarrow$ signal enhancement. For a more detailed argument see Athey and Imbens [@athey2019generalized] or Nie and Wager [@nie2017quasi]. We can still use our avoid function `gest_snmm` for efficient g-estimation, but rather than passing in the actual y we pass in $\tilde{y} = y - E(Y|A=0,H=h)$ which we can estimate using an approach of choice.

# Non-linear effect modification

So far we have talked about and simulated linear $\tau(h)$, but what if the effect modification is non-linear? Restricting ourselves to linear models in the age of machine learning seems so uncool. I'll add that many of the ideas underlying g-estimation and snmm come up in gradient trees (Athey @athey2019generalized) or R-learning (Nie & Wager @nie2017quasi). As a first step towards non-linear models for $\tau$ we'll consider approaches that involve transformations of the design matrix $H$ (the matrix containing our effect modifiers and generally a vector of 1s for the main (first order) effect of treatment). So we have a transformation $\Phi: \mathbb{R}^{n \times p} \to \mathbb{R}^{n \times q}$, including methods such as the Fourier transform, polynomial expanstions, spline basis matrices. We then replace $X$ with $\Phi(H)$ in our estimation procedure

$$
\begin{align}
\hat{\boldsymbol{\psi}}_e &= (\boldsymbol{\Phi}^t\textbf{W}\textbf{A}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^t\textbf{W}(\textbf{y}-\hat{m}_0)
\end{align}
$$

To illustrate this, lets do another simulation. In this case $\tau$ is non-linear. We have increased the number of variables to two (clearly the simulations should not be relied upon).

For this simulation I've increased the sample size to $N=1000$.

```{r echo=TRUE}
## SIMULATION 3
M <- 500  # number of runs
tauM <- replicate(M, {
  # generate data
  N <- 1000
  x1 <- runif(N,-0.5,3)
  x2 <- rnorm(N,2,3)
  ps <- function(x1,x2) 1/(1 + exp(2 - 1.8*x1 + 0.2*x2))
  a <- 1*(ps(x1,x2) < runif(N))
  psi0 <- 2.5
  psi1 <- 1.5
  tau <- psi0 + 0.7*exp(x1) + 0.5*x2 + 0.4*x2^2 + rnorm(N)
  y <- 1.2*x2 - 1.4*x2^2 + 0.8*exp(x1) + a*tau + rnorm(N,sd = 3)
  # estimate probability of treatment
  pm <- glm(a ~ x1 + x2,family = binomial())
  Xs <-  cbind(rep(1,N),bs(x1),bs(x2))
  ph <- fitted(pm)
  # estimate treatment effect
  ols <- lm(y ~ bs(x1) + bs(x2) + a + bs(x1)*a + bs(x2)*a)
  df0 <- data.frame(x1,x2,a=0)
  df1 <- data.frame(x1,x2,a=1)
  tau_ols <- predict(ols,df1) - predict(ols,df0)
  g <- gest_snmm(Xs,y - predict(ols,df0),a,ph)
  tau_g <- as.vector(Xs %*% g)
  c(bias_g=mean((tau_g-tau)),
    bias_l=(mean((tau_ols-tau))),
    mse_g=mean((tau_g-tau)^2),
    mse_l=(mean((tau_ols-tau)^2)))
})
```


```{r,fig.cap="Average bias of individual treatment effect"}
# plot results
tau_df <- data.frame(t(tauM))
bias_df <- tau_df[,c("bias_g","bias_l")]
mse_df <- tau_df[,c("mse_g","mse_l")]
bias_df <- pivot_longer(bias_df,1:2,names_to = "method",
                       values_to = "bias")
mse_df <- pivot_longer(mse_df,1:2,names_to = "method",
                       values_to = "mse")
method_labs <- c("SNMM","SNMM","Outcome model","Outcome model")
names(method_labs) <- c("bias_g","mse_g","bias_l","mse_l")
ggplot(bias_df) +
  geom_histogram(aes(x = bias,group=method),
                 col="white",fill="skyblue") +
  facet_wrap(~method,ncol=1,
             labeller = labeller(method = method_labs)) +
  theme_bw() +
  labs(x = "Bias of individual treatment effect estimate")
```

```{r,fig.cap="Average meas square error of individual treatment effect"}
ggplot(mse_df) +
  geom_histogram(aes(x = mse,group=method),
                 col="white",fill="skyblue") +
  facet_wrap(~method,ncol=1,
             labeller = labeller(method = method_labs)) +
  theme_bw() +
  labs(x = "MSE of individual treatment effect estimate")
```

# Conclusion and up-next

We've done an overview of SNMM and G-estimation focusing on the single stage setting. While the models are not often used in practise according to XXX. I hope this shows that they are a good . We have largely dealt with simplistic situations to emphasise the fundamentals of the methods. G-estimation is based on setting an empirical covariance equal to zero building off the assumptions from causal inference - ignorability/unconfoundedess, consistency and positivity. As mentioned at the beginning there is much more to causal inference than the methods and often subject matter knowledge plays an important role in justifying the reasonableness of these assumptions and designing the dat collection/extraction process.

There are several things we haven't covered in this tour of SNMM and G-estimation:

* Non-continous outcomes - in particular binary outcomes
* More than point estimates - quantiles or distributions
* Treatment over time
* Simulations that really test SNMM (or it's competitors) in the complex and noisy datasets common in data science practise

We'll come back to these topics, in particular Treatment over time in subsequent posts. Thanks for reading.


